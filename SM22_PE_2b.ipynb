{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<image src = \"img/SM22_PE_2b/1.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['is', 'a', 'sample', 'text', 'with', 'some', 'in', 'it']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt')  # Download necessary resources\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Define your text\n",
    "s = \"This is a sample text with some Words in it.\"\n",
    "\n",
    "# Tokenize the text into words\n",
    "words = word_tokenize(s)\n",
    "lowercase_words = [word for word in words if word.islower()]\n",
    "\n",
    "print(lowercase_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<image src = \"img/SM22_PE_2b/2.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# from nltk.corpus import webtext\n",
    "# from nltk import FreqDist\n",
    "# #Load the Chat Corpus (text5) using the webtext corpus from NLTK:\n",
    "# chat_corpus = nltk.corpus.webtext.words(\"firefox.txt\")\n",
    "# #Use a list comprehension to filter out all the four-letter words from the corpus:\n",
    "# four_letter_words = [word for word in chat_corpus if len(word) == 4]\n",
    "# #Create a frequency distribution object from the filtered four-letter words:\n",
    "# fdist = FreqDist(four_letter_words)\n",
    "# #Finally, display the four-letter words in decreasing order of frequency using the most_common() method of the fdist object:\n",
    "# for word, frequency in fdist.most_common():\n",
    "#     print(f\"{word}: {frequency}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters\n",
    "from nltk import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def get_four_letter_word_frequencies(corpus):\n",
    "    # Tokenize the text\n",
    "    words = word_tokenize(corpus.lower())  # Convert to lower case for uniformity\n",
    "\n",
    "    # Filter for four-letter words\n",
    "    four_letter_words = [word for word in words if len(word) == 4 and word.isalpha()]\n",
    "\n",
    "    # Create a frequency distribution\n",
    "    freq_dist = FreqDist(four_letter_words)\n",
    "\n",
    "    # Print the four-letter words in decreasing order of frequency\n",
    "    for word, frequency in freq_dist.most_common():\n",
    "        print(f\"{word}: {frequency}\")\n",
    "\n",
    "# Load the Reuters corpus (or any other corpus as needed)\n",
    "text_corpus = reuters.raw()\n",
    "\n",
    "# Call the function with the loaded corpus\n",
    "get_four_letter_word_frequencies(text_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<image src = \"img/SM22_PE_2b/3.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original string:\n",
      "Joe waited for the train. The train was late. Mary and Samantha took the bus. I looked for Mary and Samantha at the bus station.\n",
      "List of words:\n",
      "['Joe', 'waited', 'for', 'the', 'train', '.', 'The', 'train', 'was', 'late', '.', 'Mary', 'and', 'Samantha', 'took', 'the', 'bus', '.', 'I', 'looked', 'for', 'Mary', 'and', 'Samantha', 'at', 'the', 'bus', 'station', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "\n",
    "def create_word_list(string):\n",
    "    # Tokenize the string into words\n",
    "    words = nltk.word_tokenize(string)\n",
    "\n",
    "    # Return the list of words\n",
    "    return words\n",
    "\n",
    "# Define the input string\n",
    "input_string = \"Joe waited for the train. The train was late. Mary and Samantha took the bus. I looked for Mary and Samantha at the bus station.\"\n",
    "\n",
    "# Call the function to create the word list\n",
    "word_list = create_word_list(input_string)\n",
    "\n",
    "# Print the original string\n",
    "print(\"Original string:\")\n",
    "print(input_string)\n",
    "\n",
    "# Print the list of words\n",
    "print(\"List of words:\")\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<image src = \"img/SM22_PE_2b/4.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robert Langdon\n",
      "is\n",
      "a\n",
      "famous\n",
      "character\n",
      "in\n",
      "various\n",
      "books\n",
      "and\n",
      "movies\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#Tokenize the given sentence into individual words:\n",
    "text = \"Robert Langdon is a famous character in various books and movies\"\n",
    "tokens = word_tokenize(text)\n",
    "#Merge the first name and last name into a single token:\n",
    "\n",
    "merged_tokens = []\n",
    "i = 0\n",
    "while i < len(tokens):\n",
    "    if i + 1 < len(tokens) and tokens[i + 1].istitle():  # Check if the next token starts with a capital letter\n",
    "        merged_tokens.append(tokens[i] + \" \" + tokens[i + 1])\n",
    "        i += 2  # Skip the next token since it has already been merged\n",
    "    else:\n",
    "        merged_tokens.append(tokens[i])\n",
    "        i += 1\n",
    "#Print the desired output:\n",
    "for token in merged_tokens:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Additional`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<image src = \"img/SM22_PE_2b/5.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_not_in_vocabulary(text, vocabulary):\n",
    "    # Convert lists to sets\n",
    "    text_set = set(text)\n",
    "    vocab_set = set(vocabulary)\n",
    "    \n",
    "    # Find words in text that are not in the vocabulary\n",
    "    result_set = text_set - vocab_set\n",
    "    \n",
    "    return result_set\n",
    "\n",
    "# Example usage\n",
    "text = [\"hello\", \"world\", \"this\", \"is\", \"a\", \"test\"]\n",
    "vocabulary = [\"hello\", \"is\", \"a\"]\n",
    "\n",
    "# Get the set of words in the text but not in the vocabulary\n",
    "result = words_not_in_vocabulary(text, vocabulary)\n",
    "\n",
    "print(result)  # Output: {'world', 'this', 'test'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<image src = \"img/SM22_PE_2b/6.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word            Meaning                                  Pronunciation       \n",
      "===========================================================================\n",
      "apple           A round fruit with red or green skin and a whitish interior. /ˈæp.əl/            \n",
      "banana          A long, curved fruit with a yellow skin and soft, sweet, white flesh inside. /bəˈnæn.ə/          \n",
      "cherry          A small, round fruit with red or black skin and a pit. /ˈtʃɛr.i/           \n"
     ]
    }
   ],
   "source": [
    "def print_lexicon_index(lexicon):\n",
    "    # Print headers\n",
    "    print(f\"{'Word':<15} {'Meaning':<40} {'Pronunciation':<20}\")\n",
    "    print(\"=\"*75)\n",
    "    \n",
    "    # Print each lexical entry\n",
    "    for entry in lexicon:\n",
    "        word = entry.get('word', '')\n",
    "        meaning = entry.get('meaning', '')\n",
    "        pronunciation = entry.get('pronunciation', '')\n",
    "        print(f\"{word:<15} {meaning:<40} {pronunciation:<20}\")\n",
    "\n",
    "# Example lexicon\n",
    "lexicon = [\n",
    "    {\"word\": \"apple\", \"meaning\": \"A round fruit with red or green skin and a whitish interior.\", \"pronunciation\": \"/ˈæp.əl/\"},\n",
    "    {\"word\": \"banana\", \"meaning\": \"A long, curved fruit with a yellow skin and soft, sweet, white flesh inside.\", \"pronunciation\": \"/bəˈnæn.ə/\"},\n",
    "    {\"word\": \"cherry\", \"meaning\": \"A small, round fruit with red or black skin and a pit.\", \"pronunciation\": \"/ˈtʃɛr.i/\"}\n",
    "]\n",
    "\n",
    "# Print the lexicon index\n",
    "print_lexicon_index(lexicon)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check StopWord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Các từ dừng (stopwords) trong tiếng Anh:\n",
      "{'under', 'off', 'itself', 'yours', 'were', 'most', 'through', 'further', 'yourselves', \"you'd\", 'their', \"didn't\", 'can', 'over', 'than', 'up', 'each', 'isn', 'nor', \"hasn't\", 'down', 'no', 'wasn', 'ours', 'during', 'too', 'ourselves', 'having', 'weren', 'same', 'we', 'is', \"weren't\", 'not', 'who', 'its', 'why', 'how', 'o', \"shan't\", 'between', 'couldn', 'myself', 'her', 'whom', 'to', 'she', 'where', 'while', 'your', 'other', 'that', 'they', 'an', 'did', \"that'll\", 'haven', 'some', 'by', \"it's\", 'y', 'just', 'you', 'the', 'theirs', 'our', 'd', 'll', \"wasn't\", 'ma', \"should've\", 'i', 'any', \"doesn't\", 'these', 'or', 'more', 'what', 'been', 'it', 'won', 'again', 'so', 'themselves', 'but', \"don't\", \"hadn't\", \"shouldn't\", 're', 'have', 'before', 'there', 'as', \"you're\", 'him', 'shouldn', 'out', 'own', 'once', 'was', 'being', 'his', 'don', 'for', \"mustn't\", 'if', 'all', 'very', 'my', 'he', 'above', 'when', 'mustn', 'yourself', 'does', 'those', 'and', 'in', 'only', 'aren', 'into', 'hers', 'should', 'wouldn', 'be', 'about', 'are', 'on', 'then', \"she's\", 'ain', 'didn', 'himself', \"you'll\", 'hadn', 'from', \"haven't\", 'me', 'few', 'this', 'a', 'doesn', 'because', 'below', 'such', 'here', 'which', 'am', \"you've\", \"mightn't\", 'now', 'at', 'after', \"aren't\", 's', 'hasn', \"isn't\", 'm', \"needn't\", 'both', 'with', 'shan', \"won't\", 'against', 'do', 'had', 'will', 'needn', 'of', \"wouldn't\", 'herself', 'them', 'has', 've', \"couldn't\", 't', 'doing', 'until', 'mightn'}\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# # Tải các stopwords\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# Lấy danh sách các stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Kiểm tra các stopwords\n",
    "print(\"Các từ dừng (stopwords) trong tiếng Anh:\")\n",
    "print(stop_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
