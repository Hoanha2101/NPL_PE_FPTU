{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "<image src = \"img\\2_5\\1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Để khớp tất cả các chuỗi chữ cái thường bắt đầu bằng chữ 'p', bạn có thể sử dụng biểu thức chính quy sau:\n",
    "\n",
    "^p[a-z]*$\n",
    "\n",
    "Explanation:\n",
    "\n",
    "+ ^: Asserts the start of the string.\n",
    "+ p: Matches the lowercase letter p at the beginning of the string.\n",
    "+ [a-z]*: Matches zero or more lowercase alphabetic characters following p.\n",
    "+ $: Asserts the end of the string.\n",
    "\n",
    "\n",
    "Để khớp các chuỗi bắt đầu bằng một từ ở đầu dòng và kết thúc bằng một số nguyên ở cuối dòng, bạn có thể sử dụng biểu thức chính quy sau:\n",
    "\n",
    "^\\w+.*\\d+$\n",
    "\n",
    "\n",
    "Explanation:\n",
    "\n",
    "+ ^: Asserts the start of the string.\n",
    "+ \\w+: Matches one or more word characters (letters, digits, or underscores) at the beginning of the string. This captures the initial word.\n",
    "+ .*: Matches zero or more of any character (except for newline characters). This allows for any characters between the word and the number.\n",
    "+ \\d+: Matches one or more digits at the end of the string.\n",
    "+ $: Asserts the end of the string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "<image src = \"img\\2_5\\2.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ban: 1\n",
      "cac: 1\n",
      "chao: 2\n",
      "nam: 1\n",
      "viet: 1\n",
      "xin: 1\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "\n",
    "punc = string.punctuation\n",
    "\n",
    "def remove_punc(text):\n",
    "    for char in punc:\n",
    "        text = text.replace(char,\"\")\n",
    "    \n",
    "    return text\n",
    "\n",
    "text = 'xin chao viet nam, chao cac ban'\n",
    "\n",
    "text_processing = remove_punc(text)\n",
    "text_processing_list = text_processing.split()\n",
    "\n",
    "unique_word = list(np.unique(text_processing_list))\n",
    "unique_word.sort()\n",
    "\n",
    "count_dict = {}\n",
    "\n",
    "for i in unique_word:\n",
    "    count_dict[i] = 0\n",
    "    for word in text_processing_list:\n",
    "        if word == i:\n",
    "            count_dict[i] = count_dict[i] + 1\n",
    "for key, value in count_dict.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "<image src = \"img\\2_5\\3.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "\n",
    "punc = string.punctuation\n",
    "text = 'xin chao viet nam, chao cac ban'\n",
    "\n",
    "def most_char(text):\n",
    "    list_text = []\n",
    "    text_list = text.split()\n",
    "    for i in text_list:\n",
    "        list_text.append(i[0])\n",
    "    list_unique = list(np.unique(list_text))\n",
    "    \n",
    "    COUNT = []\n",
    "\n",
    "    for i in range(len(list_unique)):\n",
    "        count = 0\n",
    "        for char in text:\n",
    "            if list_unique[i] == char:\n",
    "                count += 1 \n",
    "        COUNT.append(count)\n",
    "    return list_unique[COUNT.index(max(COUNT))]\n",
    "\n",
    "most_char(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def most_common_starting_char(sentence):\n",
    "    # Split the sentence into words\n",
    "    words = sentence.split()\n",
    "    \n",
    "    # Extract the first character of each word\n",
    "    starting_chars = [word[0].lower() for word in words]\n",
    "    \n",
    "    # Count the frequency of each starting character\n",
    "    char_count = Counter(starting_chars)\n",
    "    \n",
    "    # Find the character with the highest frequency\n",
    "    most_common_char = char_count.most_common(1)[0][0]\n",
    "    \n",
    "    return most_common_char\n",
    "\n",
    "# Example usage\n",
    "input_sentence = 'xin chao viet nam, chao cac ban'\n",
    "output_char = most_common_starting_char(input_sentence)\n",
    "print(output_char)  # Output: c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "<image src = \"img\\2_5\\4.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.285714285714285"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'xin chao viet nam, chao cac ban'\n",
    "\n",
    "def percent_containing_string(words, substring):\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    tokens = words.split()\n",
    "    \n",
    "    for tk in tokens:\n",
    "        if substring in tk:\n",
    "            count += 1\n",
    "    \n",
    "    return 100*(count/len(tokens))\n",
    "\n",
    "percent_containing_string(text, \"am\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between text1 and text2 is 0.1707761131901165\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [stemmer.stem(word) for word in words if word.lower() not in stop_words]\n",
    "\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "\n",
    "def calculate_similarity(text1, text2):\n",
    "    preprocessed_text1 = preprocess_text(text1)\n",
    "    preprocessed_text2 = preprocess_text(text2)\n",
    "\n",
    "    corpus = [preprocessed_text1, preprocessed_text2]\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])\n",
    "\n",
    "    return cosine_sim[0][0]\n",
    "\n",
    "\n",
    "# Input texts\n",
    "text1 = \"John lives in Canada\"\n",
    "text2 = \"James lives in America, though he's not from there\"\n",
    "\n",
    "# Calculate similarity\n",
    "similarity = calculate_similarity(text1, text2)\n",
    "\n",
    "# Print results\n",
    "print(f\"Similarity between text1 and text2 is {similarity}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
