{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<image src = \"img/SM22_PE_1/1.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['realize', 'customize', 'organize', 'prioritize']\n"
     ]
    }
   ],
   "source": [
    "#Q1\n",
    "import nltk\n",
    "# realize analyze customize organize prioritize\n",
    "s = input(\"Enter your text: \")\n",
    "words = nltk.word_tokenize(s)\n",
    "\n",
    "ize_words = [word for word in words if word.endswith('ize')]\n",
    "\n",
    "print(ize_words)\n",
    "\n",
    "\n",
    "#Q1_1. Write expressions for finding all words in s that starting in \"ize\"\n",
    "# import nltk\n",
    "# s = input(\"Enter the text: \")\n",
    "# words = nltk.word_tokenize(s)\n",
    "# ize_words = [word for word in words if word.startswith('ize')]\n",
    "# print(ize_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<image src = \"img/SM22_PE_1/2.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('language', 2), ('NLP', 2), ('AI', 2), ('Natural', 1), ('processing', 1), ('field', 1), ('artificial', 1), ('intelligence', 1), ('enables', 1), ('computers', 1), ('analyze', 1), ('understand', 1), ('human', 1), ('intersection', 1), ('computer', 1), ('science', 1), ('linguistics', 1), ('wide', 1), ('range', 1), ('applications', 1), ('including', 1), ('machine', 1), ('translation', 1), ('sentiment', 1), ('analysis', 1), ('text', 1), ('summarization', 1), ('chatbots', 1)]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "\n",
    "# Ensure to download the 'stopwords' package if not already downloaded\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "def fifty_most_common(text):\n",
    "    # Tokenize the text\n",
    "    tokenized_words = nltk.word_tokenize(text)\n",
    "\n",
    "    # Filter out stopwords and punctuation\n",
    "    filtered_words = [word for word in tokenized_words if word not in stopwords.words('english') and word.isalpha()]\n",
    "\n",
    "    # Get the frequency distribution of the words\n",
    "    freqDist = FreqDist(filtered_words)\n",
    "\n",
    "    # Get the 50 most common words\n",
    "    most_common = freqDist.most_common(50)\n",
    "\n",
    "    return most_common\n",
    "\n",
    "s = input(\"Enter your text: \")\n",
    "print(fifty_most_common(s))\n",
    "\n",
    "\n",
    "\"\"\"Natural language processing (NLP) is a field of artificial intelligence (AI) \n",
    "that enables computers to analyze and understand human language. NLP is at \n",
    "the intersection of computer science, linguistics, and AI, and it has a wide \n",
    "range of applications including machine translation, sentiment analysis, \n",
    "text summarization, and chatbots.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('AI', 2), ('NLP', 2), ('language', 2), ('Natural', 1), ('analysis', 1), ('analyze', 1), ('applications', 1), ('artificial', 1), ('chatbots', 1), ('computer', 1), ('computers', 1), ('enables', 1), ('field', 1), ('human', 1), ('including', 1), ('intelligence', 1), ('intersection', 1), ('linguistics', 1), ('machine', 1), ('processing', 1), ('range', 1), ('science', 1), ('sentiment', 1), ('summarization', 1), ('text', 1), ('translation', 1), ('understand', 1), ('wide', 1)]\n"
     ]
    }
   ],
   "source": [
    "# Cách 2:\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "\n",
    "def fifty_most_common(text, num):\n",
    "    # Tokenize the text\n",
    "    tokenized_words = nltk.word_tokenize(text)\n",
    "\n",
    "    # Filter out stopwords and punctuation\n",
    "    filtered_words = [word for word in tokenized_words if word not in stopwords.words('english') and word.isalpha()]\n",
    "    \n",
    "    uni_word = np.unique(filtered_words)\n",
    "    \n",
    "    Uni_Dict = {}\n",
    "    \n",
    "    for i in uni_word:\n",
    "        Uni_Dict[i] = 0\n",
    "        for j in filtered_words:\n",
    "            if i == j:\n",
    "                Uni_Dict[i] = Uni_Dict[i] + 1  \n",
    "    Sorted = list(sorted(Uni_Dict.items(), key=lambda item: item[1], reverse=True))\n",
    "    if len(Sorted) < num:\n",
    "        num = len(Sorted)   \n",
    "    return Sorted[:num]\n",
    "\n",
    "s = \"\"\"Natural language processing (NLP) is a field of artificial intelligence (AI) \n",
    "that enables computers to analyze and understand human language. NLP is at \n",
    "the intersection of computer science, linguistics, and AI, and it has a wide \n",
    "range of applications including machine translation, sentiment analysis, \n",
    "text summarization, and chatbots.\"\"\"\n",
    "\n",
    "print(fifty_most_common(s, 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<image src = \"img/SM22_PE_1/3.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['your', 'password', 'if', 'you', 'just', 'can', \"'\", 't', 'remember', 'your', 'old', 'one', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "text = \"your password if you just can't remember your old one.\"\n",
    "tokenized_text = wordpunct_tokenize(text)\n",
    "\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<image src = \"img/SM22_PE_1/4.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Bài này không rõ dùng hàm nào để tính độ tương đồng\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between text1 and text2 is 0.20608363501393823\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "text1 = \"John lives in Canada\"\n",
    "text2 = \"James lives in America, though he's not from there\"\n",
    "\n",
    "# Create the Document Term Matrix\n",
    "vectorizer = TfidfVectorizer()\n",
    "transformed_vector = vectorizer.fit_transform([text1, text2])\n",
    "\n",
    "similarity_score = cosine_similarity(transformed_vector[0:1], transformed_vector[1:2])\n",
    "print(\"Similarity between text1 and text2 is\", similarity_score[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between text1 and text2 is 0.299\n"
     ]
    }
   ],
   "source": [
    "# Import nltk and download wordnet\n",
    "import nltk\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# Define two texts\n",
    "text1 = \"John lives in Canada\"\n",
    "text2 = \"James lives in America, though he's not from there\"\n",
    "\n",
    "# Tokenize and stem the texts\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "tokens1 = [stemmer.stem(token) for token in tokenizer.tokenize(text1)]\n",
    "tokens2 = [stemmer.stem(token) for token in tokenizer.tokenize(text2)]\n",
    "\n",
    "# Get the wordnet synsets for each token\n",
    "synsets1 = [nltk.corpus.wordnet.synsets(token)[0] for token in tokens1 if nltk.corpus.wordnet.synsets(token)]\n",
    "synsets2 = [nltk.corpus.wordnet.synsets(token)[0] for token in tokens2 if nltk.corpus.wordnet.synsets(token)]\n",
    "\n",
    "# Compute the average wup similarity score between the texts\n",
    "similarity = 0\n",
    "count = 0\n",
    "for synset1 in synsets1:\n",
    "    for synset2 in synsets2:\n",
    "        score = synset1.wup_similarity(synset2)\n",
    "        if score:\n",
    "            similarity += score\n",
    "            count += 1\n",
    "similarity /= count\n",
    "\n",
    "# Print the similarity score\n",
    "print(f\"Similarity between text1 and text2 is {similarity:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between text1 and text2 is 0.20608363501393823\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def calculate_similarity(text1, text2):\n",
    "    # Create a TfidfVectorizer object\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # Convert the texts to TF-IDF vectors\n",
    "    tfidf_matrix = vectorizer.fit_transform([text1, text2])\n",
    "\n",
    "    # Calculate cosine similarity between the two vectors\n",
    "    similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\n",
    "\n",
    "    # Return the similarity score (as a float)\n",
    "    return similarity[0][0]\n",
    "\n",
    "# Example usage\n",
    "text1 = \"John lives in Canada\"\n",
    "text2 = \"James lives in America, though he's not from there\"\n",
    "\n",
    "similarity_score = calculate_similarity(text1, text2)\n",
    "print(f\"Similarity between text1 and text2 is {similarity_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
